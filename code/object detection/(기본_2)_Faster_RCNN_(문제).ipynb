{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8FxqpVNUBVZ"
      },
      "source": [
        "# (ê¸°ë³¸-2) Object Detection\n",
        "#### Faster RCNN êµ¬í˜„í•˜ê¸°\n",
        "Faster RCNN ì€ object detecion ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì•„í‚¤í…ì³ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ Faster RCNN ì„ í•™ìŠµí•´ë³´ê³  ì¶”ë¡ í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ë´…ì‹œë‹¤.\n",
        "\n",
        "\n",
        "### ê³¼ì œ ê°œìš”\n",
        "1. ê°•ì˜ ì‹œê°„ì— ë‹¤ë¤˜ë˜ 2stage detectorì¤‘ í•˜ë‚˜ì¸ Faster RCNNì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "2. anchor_boxë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "3. Faster RCNN ì˜ ì¤‘ìš” ìš”ì†Œì¤‘ í•˜ë‚˜ì¸ region proposal networkë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "### ê³¼ì œ ì¶œì œ ëª©ì  ë° ë°°ê²½\n",
        "Faster RCNN ì€ 2-stage object detecion ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì•„í‚¤í…ì³ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. Faster RCNNì„ í•™ìŠµ ë° ì¶”ë¡ í•˜ëŠ”ë° ìˆì–´ì„œ ì¤‘ìš”í•œ ìš”ì†Œì¸ anchor box, region proposal network ë¥¼ êµ¬í˜„í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ ë´…ì‹œë‹¤.\n",
        "\n",
        "### ê³¼ì œ ìˆ˜í–‰ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ì—­ëŸ‰  \n",
        "* anchor bboxë¥¼ ì§ì ‘ êµ¬í˜„í•´ë´„ìœ¼ë¡œì¨, anchor bboxì˜ êµ¬ì„± ìš”ì†Œ, ì—­í• ë“±ì— ëŒ€í•´ì„œ ê¹Šì€ ì´í•´ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
        "* region proposal networkë¥¼ ì§ì ‘ êµ¬í˜„í•´ë´„ìœ¼ë¡œì¨, object detection ëª¨ë¸ì—ì„œ ì¤‘ìš” ì—­í• ì„ í•˜ëŠ” region proposal network ì— ì—­í• , êµ¬ì„±ìš”ì†Œ ë“± ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### ìˆœì„œ  \n",
        "1. anchor bbox êµ¬í˜„\n",
        "2. region proposal network êµ¬í˜„\n",
        "\n",
        "\n",
        "\n",
        "> **ANSWER HERE** ì´ë¼ê³  ì‘ì„±ëœ ë¶€ë¶„ì„ ì±„ì›Œ ì™„ì„±í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë‹¤ë¥¸ ë¶€ë¶„ì˜ ì½”ë“œë¥¼ ë³€ê²½í•˜ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "#### Faster RCNN êµ¬í˜„í•˜ê¸°\n",
        "<br>Faster RCNNì˜ ìì„¸í•œ ë‚´ìš©ì€ 02ê°•: 2stage Detectors ê°•ì˜ë¥¼ ì°¸ê³ í•©ë‹ˆë‹¤."
      ],
      "id": "w8FxqpVNUBVZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfqy0tJJUBVc"
      },
      "source": [
        "## ëŒ€íšŒ ë°ì´í„°ì…‹ êµ¬ì„±\n",
        "ì½”ë“œë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë°›ì•„ì¤ë‹ˆë‹¤\n",
        "**wget https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000266/data/data.tar.gz**\n",
        "\n",
        "ë°ì´í„°ì…‹ì˜ ìì„¸í•œ ê°œìš”ëŠ” [ëŒ€íšŒ í”Œë«í¼](https://stages.ai/competitions/325/data/overview)ì˜ ë°ì´í„° ì„¤ëª…ì„ ì°¸ê³ í•©ë‹ˆë‹¤.\n",
        "> Copyright: CC BY 2.0\n",
        "\n",
        "### dataset\n",
        "    â”œâ”€â”€ train.json\n",
        "    â”œâ”€â”€ test.json\n",
        "    â”œâ”€â”€ train\n",
        "    â””â”€â”€ test"
      ],
      "id": "nfqy0tJJUBVc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsgTBr3BqE8z"
      },
      "source": [
        "# Library Import"
      ],
      "id": "xsgTBr3BqE8z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEa9vTBhUBVd"
      },
      "outputs": [],
      "source": [
        "# ê¶Œì¥ í™˜ê²½: python==3.7.13, pytorch==1.13.1, torchvision==0.14.1, albumentations==1.3.1, torchnet==0.0.4"
      ],
      "id": "JEa9vTBhUBVd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce329765"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "id": "ce329765"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54d76dda"
      },
      "outputs": [],
      "source": [
        "# !pip install visdom"
      ],
      "id": "54d76dda"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c142ed1a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import six\n",
        "from collections import namedtuple\n",
        "\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision.models import vgg16\n",
        "from torchvision.ops import RoIPool\n",
        "from torchvision.ops import nms\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils import data as data_\n",
        "\n",
        "from torchnet.meter import ConfusionMeter, AverageValueMeter"
      ],
      "id": "c142ed1a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wb--qP79Xx2"
      },
      "source": [
        "# Util Functions"
      ],
      "id": "2wb--qP79Xx2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebae2a20"
      },
      "outputs": [],
      "source": [
        "def loc2bbox(src_bbox, loc):\n",
        "    \"\"\"\n",
        "    Decodes bouding boxes from bounding box offsets and scales.\n",
        "\n",
        "    Args:\n",
        "        src_bbox: A coordinates of bounding boxes.\n",
        "            These coordinates are (p_ymin, p_xmin, p_ymax, p_xmax).\n",
        "        loc: An array with offsets and scales.\n",
        "            The shapes of 'src_bbox' and 'loc' should be same.\n",
        "            This contains values: (t_y, t_x, t_h, t_w).\n",
        "    Returns: Decoded bounding box coordinates.\n",
        "    \"\"\"\n",
        "\n",
        "    if src_bbox.shape[0] == 0:\n",
        "        return np.zeros((0, 4), dtype=loc.dtype)\n",
        "\n",
        "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n",
        "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
        "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
        "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
        "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
        "\n",
        "    dy = loc[:, 0::4]\n",
        "    dx = loc[:, 1::4]\n",
        "    dh = loc[:, 2::4]\n",
        "    dw = loc[:, 3::4]\n",
        "\n",
        "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
        "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
        "    h = np.exp(dh) * src_height[:, np.newaxis]\n",
        "    w = np.exp(dw) * src_width[:, np.newaxis]\n",
        "\n",
        "    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n",
        "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
        "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
        "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
        "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
        "\n",
        "    return dst_bbox\n",
        "\n",
        "\n",
        "def bbox2loc(src_bbox, dst_bbox):\n",
        "    \"\"\"\n",
        "    Encodes the source and the destination bouding boxes to \"loc\".\n",
        "\n",
        "    The offsets and scales t_y, t_x, t_h, t_w can be computed by the following formulas\n",
        "    t_y = (g_y - p_y) / p_h\n",
        "    t_x = (g_x - p_x) / p_w\n",
        "    t_h = log(g_h / p_h)\n",
        "    t_w = log(g_w / p_W)\n",
        "\n",
        "    Args:\n",
        "        src_bbox: These coordinates are (p_ymin, p_xmin, p_ymax, p_xmax).\n",
        "        dst_bbox: These coordinates are (g_ymin, g_xmin, g_ymax, g_xmax).\n",
        "\n",
        "    Returns:\n",
        "        Bounding box offsets and scales from src_bbox to dst_bbox.\n",
        "        The second axis contains four values (t_y, t_x, t_h, t_w).\n",
        "    \"\"\"\n",
        "\n",
        "    # x_min, y_min, x_max, y_max\n",
        "    height = src_bbox[:, 2] - src_bbox[:, 0]\n",
        "    width = src_bbox[:, 3] - src_bbox[:, 1]\n",
        "    ctr_y = src_bbox[:, 0] + 0.5 * height\n",
        "    ctr_x = src_bbox[:, 1] + 0.5 * width\n",
        "\n",
        "    # x_min, y_min, x_max, y_max\n",
        "    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n",
        "    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n",
        "    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n",
        "    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n",
        "\n",
        "    eps = np.finfo(height.dtype).eps\n",
        "    height = np.maximum(height, eps)\n",
        "    width = np.maximum(width, eps)\n",
        "\n",
        "    dy = (base_ctr_y - ctr_y) / height\n",
        "    dx = (base_ctr_x - ctr_x) / width\n",
        "    dh = np.log(base_height / height)\n",
        "    dw = np.log(base_width / width)\n",
        "\n",
        "    loc = np.vstack((dy, dx, dh, dw)).transpose()\n",
        "    return loc"
      ],
      "id": "ebae2a20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DTpLq7_9Qr2"
      },
      "outputs": [],
      "source": [
        "def normal_init(m, mean, stddev, truncated=False):\n",
        "    \"\"\"\n",
        "    weight initialization\n",
        "    \"\"\"\n",
        "    if truncated:\n",
        "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)\n",
        "    else:\n",
        "        m.weight.data.normal_(mean, stddev)\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def get_inside_index(anchor, H, W):\n",
        "    # Calc indicies of anchors which are located completely inside of the image\n",
        "    # whose size is speficied.\n",
        "    index_inside = np.where(\n",
        "        (anchor[:, 0] >= 0) &\n",
        "        (anchor[:, 1] >= 0) &\n",
        "        (anchor[:, 2] <= H) &\n",
        "        (anchor[:, 3] <= W)\n",
        "    )[0]\n",
        "    return index_inside\n",
        "\n",
        "\n",
        "def unmap(data, count, index, fill=0):\n",
        "    # Unmap a subset of item (data) back to the original set of items (of size count)\n",
        "    if len(data.shape) == 1:\n",
        "        ret = np.empty((count,), dtype=data.dtype)\n",
        "        ret.fill(fill)\n",
        "        ret[index] = data\n",
        "    else:\n",
        "        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype)\n",
        "        ret.fill(fill)\n",
        "        ret[index, :] = data\n",
        "    return ret"
      ],
      "id": "1DTpLq7_9Qr2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e49054b"
      },
      "outputs": [],
      "source": [
        "def tonumpy(data):\n",
        "    if isinstance(data, np.ndarray):\n",
        "        return data\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        return data.detach().cpu().numpy()\n",
        "\n",
        "def totensor(data, cuda = True):\n",
        "    if isinstance(data, np.ndarray):\n",
        "        tensor = torch.from_numpy(data)\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        tensor = data.detach()\n",
        "    if cuda:\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "def scalar(data):\n",
        "    if isinstance(data, np.ndarray):\n",
        "        return data.reshape(1)[0]\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        return data.item()"
      ],
      "id": "7e49054b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJhVDlP3qA7h"
      },
      "source": [
        "# Main"
      ],
      "id": "PJhVDlP3qA7h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ecf14d"
      },
      "source": [
        "### í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¸íŒ…"
      ],
      "id": "e0ecf14d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "166d1663"
      },
      "outputs": [],
      "source": [
        "epochs=14\n",
        "learning_rate = 1e-3\n",
        "lr_decay = 0.1\n",
        "weight_decay = 0.0005\n",
        "# use dropout in RoIHead\n",
        "use_drop = False\n",
        "\n",
        "rpn_sigma = 3.     # sigma for l1_smooth_loss (RPN loss)\n",
        "roi_sigma = 1.     # sigma for l1_smooth_loss (ROI loss)\n",
        "\n",
        "# ë°ì´í„° ê²½ë¡œ\n",
        "data_dir = '../../dataset'\n",
        "# trainì‹œ checkpoint ê²½ë¡œ\n",
        "train_load_path = None\n",
        "# inferenceì‹œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ\n",
        "inf_load_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth'"
      ],
      "id": "166d1663"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49fb9a7d"
      },
      "source": [
        "### Dataset ë§Œë“¤ê¸°"
      ],
      "id": "49fb9a7d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a093db22"
      },
      "source": [
        "#### 1. custom data ë¶ˆëŸ¬ ì˜¤ê¸°"
      ],
      "id": "a093db22"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e08cf31"
      },
      "outputs": [],
      "source": [
        "# TrainDataset\n",
        "class TrainCustom(Dataset):\n",
        "    def __init__(self, annotation, data_dir, transforms = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            annotation: annotation íŒŒì¼ ìœ„ì¹˜\n",
        "            data_dir: dataê°€ ì¡´ì¬í•˜ëŠ” í´ë” ê²½ë¡œ\n",
        "            transforms : transform ì—¬ë¶€\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # coco annotation ë¶ˆëŸ¬ì˜¤ê¸° (coco API)\n",
        "        self.coco = COCO(annotation)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        # ì´ë¯¸ì§€ ì•„ì´ë”” ê°€ì ¸ì˜¤ê¸°\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "\n",
        "        # ì´ë¯¸ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "        image_info = self.coco.loadImgs(image_id)[0]\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "\n",
        "        # ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ë¡œë“œ\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # ë°•ìŠ¤ ê°€ì ¸ì˜¤ê¸°\n",
        "        boxes = np.array([x['bbox'] for x in anns])\n",
        "\n",
        "        # boxes (x_min, y_min, x_max, y_max) ê¼´ë¡œ ë³€í™˜\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "\n",
        "        # ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸°\n",
        "        labels = np.array([x['category_id'] for x in anns])\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # transform í•¨ìˆ˜ ì •ì˜\n",
        "        if self.transforms :\n",
        "            scale = 1.0  # resize scale\n",
        "            H, W, _ = image.shape\n",
        "            resize_H = int(scale * H)\n",
        "            resize_W = int(scale * W)\n",
        "            transforms = get_train_transform(resize_H, resize_W)\n",
        "        else :\n",
        "            scale = 1.0\n",
        "            transforms = no_transform()\n",
        "\n",
        "        # transform\n",
        "        sample = {\n",
        "            'image': image,\n",
        "            'bboxes': boxes,\n",
        "            'labels': labels\n",
        "        }\n",
        "        sample = transforms(**sample)\n",
        "        image = sample['image']\n",
        "        bboxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
        "        boxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
        "\n",
        "        # bboxes (x_min, y_min, x_max, y_max) -> boxes (y_min, x_min, y_max, x_max)\n",
        "        boxes[:, 0] = bboxes[:, 1]\n",
        "        boxes[:, 1] = bboxes[:, 0]\n",
        "        boxes[:, 2] = bboxes[:, 3]\n",
        "        boxes[:, 3] = bboxes[:, 2]\n",
        "\n",
        "        return image, boxes, labels, scale\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.coco.getImgIds())"
      ],
      "id": "6e08cf31"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "856a66d5"
      },
      "outputs": [],
      "source": [
        "# Test Dataset\n",
        "class TestCustom(Dataset):\n",
        "    def __init__(self, annotation, data_dir):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            annotation: annotation íŒŒì¼ ìœ„ì¹˜\n",
        "            data_dir: dataê°€ ì¡´ì¬í•˜ëŠ” í´ë” ê²½ë¡œ\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # coco annotation ë¶ˆëŸ¬ì˜¤ê¸° (coco API)\n",
        "        self.coco = COCO(annotation)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        # ì´ë¯¸ì§€ ì•„ì´ë”” ê°€ì ¸ì˜¤ê¸°\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "\n",
        "        # ì´ë¯¸ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "        image_info = self.coco.loadImgs(image_id)[0]\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        image = torch.tensor(image, dtype = torch.float).permute(2,0,1)\n",
        "\n",
        "        return image, image.shape[1:]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.coco.getImgIds())"
      ],
      "id": "856a66d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc847ee7"
      },
      "source": [
        "#### 2. Transform"
      ],
      "id": "bc847ee7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d2af672"
      },
      "outputs": [],
      "source": [
        "# Train dataset transform\n",
        "def get_train_transform(h, w):\n",
        "    return A.Compose([\n",
        "        A.Resize(height = h, width = w),\n",
        "        A.Flip(p=0.5),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "# No transform\n",
        "def no_transform():\n",
        "    return A.Compose([\n",
        "        ToTensorV2(p=1.0) # format for pytorch tensor\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "id": "6d2af672"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8a02619"
      },
      "source": [
        "### RPN (Region Proposal Network) ì •ì˜\n"
      ],
      "id": "e8a02619"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43368909"
      },
      "source": [
        "#### 1. Anchor box ìƒì„± (generate_anchor_base)\n",
        "\n",
        "ğŸ‘‰ mission1. anchor box ì¢Œí‘œê°’ ìƒì„±\\\n",
        ": 'ANSWER HERE' ì— ì½”ë“œë¥¼ ì¶”ê°€í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤!\n",
        "  (ì–´ë ¤ìš°ì‹  ê²½ìš°ì—ëŠ” ê°•ì˜ë¥¼ ì°¸ê³ í•´ì„œ ì¶”ê°€í•´ì£¼ì„¸ìš”~)\n",
        "1. ì¤‘ì  ë§Œë“¤ê¸° (base_sizeì˜ ì ˆë°˜)\n",
        "\n",
        "\n",
        "2. í•˜ë‚˜ì˜ ì¤‘ì ë‹¹ ratioì™€ anchor scalesì— ë”°ë¼ 9ê°œì˜ anchor boxì˜ ì¢Œí‘œê°’ ë§Œë“¤ê¸°\\\n",
        "    anchor boxì˜ ì¢Œí‘œê°’ : (y_min, x_min, y_max, x_max)\n"
      ],
      "id": "43368909"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2155d0ec"
      },
      "outputs": [],
      "source": [
        "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ratios: ë¹„ìœ¨\n",
        "        anchor_scales: ìŠ¤ì¼€ì¼\n",
        "    Returns: basic anchor boxes, shape=(R, 4)\n",
        "        R: len(ratio) * len(anchor_scales) = anchor ê°œìˆ˜ = 9\n",
        "        4: anchor box ì¢Œí‘œ ê°’\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    ### ANSWER HERE ###\n",
        "    # êµ¬í˜„í•´ì•¼ í•  ë³€ìˆ˜ : px, py\n",
        "      # px\n",
        "      # py\n",
        "    py =\n",
        "    px =\n",
        "\n",
        "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32) # anchor_box\n",
        "\n",
        "    for i in six.moves.range(len(ratios)):\n",
        "        for j in six.moves.range(len(anchor_scales)):\n",
        "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n",
        "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n",
        "\n",
        "            index = i * len(anchor_scales) + j\n",
        "            # offset of anchor box\n",
        "\n",
        "            ### YOUR CODE HERE\n",
        "            ### ANSWER HERE ###\n",
        "            # êµ¬í˜„í•´ì•¼ í•  ë³€ìˆ˜ : anchor_base\n",
        "            # anchor_base[index, 0]\n",
        "            # anchor_base[index, 1]\n",
        "            # anchor_base[index, 2]\n",
        "            # anchor_base[index, 3]\n",
        "            anchor_base[index, 0] =\n",
        "            anchor_base[index, 1] =\n",
        "            anchor_base[index, 2] =\n",
        "            anchor_base[index, 3] =\n",
        "\n",
        "    return anchor_base # (9,4)"
      ],
      "id": "2155d0ec"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e74ef20d"
      },
      "source": [
        "#### 2. Proposal ìƒì„± (ProposalCreator)\n",
        "RPNì—ì„œ êµ¬í•œ rpn_locì™€ anchorì„ í†µí•´ì„œ Region of Interest(RoI)ë¥¼ ìƒì„±\\\n",
        "RoI ê°œìˆ˜ ì¤„ì´ê¸° ìœ„í•´ì„œ ë¯¸ë¦¬ ì •í•´ë‘” í¬ê¸°(min_size)ì— ë§ëŠ” roië“¤ ì¤‘ NMSë¥¼ í†µí•´ ìµœì¢… RoI ë°˜í™˜ (train ì‹œ 2000ê°œ)"
      ],
      "id": "e74ef20d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4af7541e"
      },
      "outputs": [],
      "source": [
        "class ProposalCreator:\n",
        "    def __init__(self, parent_model,\n",
        "                 nms_thresh=0.7, # nms threshold\n",
        "                 n_train_pre_nms=12000, # trainì‹œ nms ì „ roi ê°œìˆ˜\n",
        "                 n_train_post_nms=2000, # trainì‹œ nms í›„ roi ê°œìˆ˜\n",
        "                 n_test_pre_nms=6000,   # testì‹œ nms ì „ roi ê°œìˆ˜\n",
        "                 n_test_post_nms=300,   # testì‹œ nms í›„ roi ê°œìˆ˜\n",
        "                 min_size=16\n",
        "                 ):\n",
        "        self.parent_model = parent_model\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.n_train_pre_nms = n_train_pre_nms\n",
        "        self.n_train_post_nms = n_train_post_nms\n",
        "        self.n_test_pre_nms = n_test_pre_nms\n",
        "        self.n_test_post_nms = n_test_post_nms\n",
        "        self.min_size = min_size\n",
        "\n",
        "    def __call__(self, loc, score, anchor, img_size, scale=1.):\n",
        "        if self.parent_model.training: # trainì¤‘ì¼ ë•Œ\n",
        "            n_pre_nms = self.n_train_pre_nms\n",
        "            n_post_nms = self.n_train_post_nms\n",
        "        else: # testì¤‘ì¼ ë•Œ\n",
        "            n_pre_nms = self.n_test_pre_nms\n",
        "            n_post_nms = self.n_test_post_nms\n",
        "\n",
        "        # anchorì˜ ì¢Œí‘œê°’ê³¼ predicted bounding bounding box offset(y,x,h,w)ë¥¼ í†µí•´\n",
        "        # bounding box ì¢Œí‘œê°’(y_min, x_min, y_max, x_max) ìƒì„±\n",
        "        roi = loc2bbox(anchor, loc)\n",
        "\n",
        "        # Clip predicted boxes to image.\n",
        "        roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
        "        roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
        "\n",
        "        # min_size ë³´ë‹¤ ì‘ì€ boxë“¤ì€ ì œê±°\n",
        "        min_size = self.min_size * scale\n",
        "        hs = roi[:, 2] - roi[:, 0]\n",
        "        ws = roi[:, 3] - roi[:, 1]\n",
        "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
        "        roi = roi[keep, :]\n",
        "        score = score[keep]\n",
        "\n",
        "        # Sort all (proposal, score) pairs by score from highest to lowest.\n",
        "        # Take top pre_nms_topN\n",
        "        order = score.ravel().argsort()[::-1]\n",
        "        if n_pre_nms > 0:\n",
        "            order = order[:n_pre_nms]\n",
        "        roi = roi[order, :]\n",
        "        score = score[order]\n",
        "\n",
        "        # nms ì ìš©\n",
        "        keep = nms(\n",
        "            torch.from_numpy(roi).cuda(),\n",
        "            torch.from_numpy(score).cuda(),\n",
        "            self.nms_thresh)\n",
        "        if n_post_nms > 0:\n",
        "            keep = keep[:n_post_nms]\n",
        "        roi = roi[keep.cpu().numpy()]\n",
        "\n",
        "        return roi"
      ],
      "id": "4af7541e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a991ada"
      },
      "source": [
        "#### 3. region proposal network\n",
        "\n",
        "VGG16 í†µê³¼í•œ feature mapìœ¼ë¡œë¶€í„° region proposalë“¤ ìƒì„±\n",
        "\n",
        "ğŸ‘‰ mission2. Region Proposal Network\\\n",
        "'ANSWER HERE' ì— ì½”ë“œë¥¼ ì¶”ê°€í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤! (ì–´ë ¤ìš°ì‹  ê²½ìš° ê°•ì˜ë¥¼ ì°¸ê³ í•´ì„œ ì¶”ê°€í•´ì£¼ì„¸ìš”~)\\\n",
        " **tensor shapeì€ ëª¨ë‘ 1024x1024 ì´ë¯¸ì§€ ê¸°ì¤€ì…ë‹ˆë‹¤.**\n",
        "1. backboneì—ì„œ ë‚˜ì˜¨ feature mapì— 3x3 conv ì—°ì‚°ì„ ì ìš©í•˜ì—¬ ì¤‘ê°„ feature map ìƒì„± \\\n",
        "    input: x (torch.Size([1, 512, 64, 64]))\\\n",
        "    output: middle (torch.Size([1, 512, 64, 64]))\n",
        "    \n",
        "    \n",
        "2. middle(ì¤‘ê°„ feature map)ì— 1x1 conv ì—°ì‚°ì„ ì ìš©í•˜ì—¬ 9x4(anchor boxì˜ ìˆ˜ x bounding box ì¢Œí‘œê°’)ê°œì˜ channelì„ ê°€ì§€ëŠ” feature map ìƒì„±\\\n",
        "    input: middle (torch.Size([1, 512, 64, 64]))\\\n",
        "    output: rpn_locs (torch.Size([1, 36, 64, 64]))\n",
        "    \n",
        "    \n",
        "3. middle(ì¤‘ê°„ feature map)ì— 1x1 conv ì—°ì‚°ì„ ì ìš©í•˜ì—¬ 9x2(anchor boxì˜ ìˆ˜ x object ì—¬ë¶€)ê°œì˜ channelì„ ê°€ì§€ëŠ” feature map ìƒì„±\\\n",
        "    input: middle (torch.Size([1, 512, 64, 64]))\\\n",
        "    output: rpn_locs (torch.Size([1, 18, 64, 64]))\n",
        "    \n",
        "    \n",
        "4. Proposal Creator í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ roi ìƒì„±"
      ],
      "id": "7a991ada"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb749c00"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(nn.Module):\n",
        "    def __init__(self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
        "                 anchor_scales=[8, 16, 32], feat_stride=16, proposal_creator_params=dict(),):\n",
        "\n",
        "        super(RegionProposalNetwork, self).__init__()\n",
        "\n",
        "        self.anchor_base = generate_anchor_base(anchor_scales=anchor_scales, ratios=ratios) # 9ê°œì˜ anchorbox ìƒì„±\n",
        "        self.feat_stride = feat_stride\n",
        "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params) # proposal_creator_params : í•´ë‹¹ ë„¤íŠ¸ì›Œí¬ê°€ trainingì¸ì§€ testingì¸ì§€ ì•Œë ¤ì¤€ë‹¤.\n",
        "        n_anchor = self.anchor_base.shape[0] # anchor ê°œìˆ˜\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
        "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)  # 9*2\n",
        "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)   # 9*4\n",
        "        normal_init(self.conv1, 0, 0.01) # weight initalizer\n",
        "        normal_init(self.score, 0, 0.01) # weight initalizer\n",
        "        normal_init(self.loc, 0, 0.01)   # weight initalizer\n",
        "\n",
        "    def forward(self, x, img_size, scale=1.):\n",
        "        # x(feature map)\n",
        "        n, _, hh, ww = x.shape\n",
        "\n",
        "        # ì „ì²´ (h*w*9)ê°œ anchorì˜ ì¢Œí‘œê°’ # anchor_base:(9, 4)\n",
        "        anchor = _enumerate_shifted_anchor(np.array(self.anchor_base), self.feat_stride, hh, ww)\n",
        "        n_anchor = anchor.shape[0] // (hh * ww) # anchor ê°œìˆ˜\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "        ### ANSWER HERE ###\n",
        "        middle = F.relu('ANSWER_HERE')\n",
        "\n",
        "        # predicted bounding box offset\n",
        "        ### YOUR CODE HERE\n",
        "        ### ANSWER HERE ###\n",
        "        rpn_locs = 'ANSWER HERE'\n",
        "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n",
        "\n",
        "        # predicted scores for anchor (foreground or background)\n",
        "        ### YOUR CODE HERE\n",
        "        ### ANSWER HERE ###\n",
        "        rpn_scores = 'ANSWER HERE'\n",
        "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        # scores for foreground\n",
        "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4)\n",
        "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous()\n",
        "        rpn_fg_scores = rpn_fg_scores.view(n, -1)\n",
        "\n",
        "        rpn_scores = rpn_scores.view(n, -1, 2)\n",
        "\n",
        "        # proposalìƒì„± (ProposalCreator)\n",
        "        rois = list()        # proposalì˜ ì¢Œí‘œê°’ì´ ìˆëŠ” bounding box array\n",
        "        roi_indices = list() # roiì— í•´ë‹¹í•˜ëŠ” image ì¸ë±ìŠ¤\n",
        "        for i in range(n):\n",
        "            ### YOUR CODE HERE\n",
        "            ### ANSWER HERE ###\n",
        "            roi = 'ANSWER HERE'\n",
        "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
        "            rois.append(roi)\n",
        "            roi_indices.append(batch_index)\n",
        "        rois = np.concatenate(rois, axis=0)\n",
        "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
        "\n",
        "        return rpn_locs, rpn_scores, rois, roi_indices, anchor\n",
        "\n",
        "\n",
        "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
        "    # anchor_baseëŠ” í•˜ë‚˜ì˜ pixelì— 9ê°œ ì¢…ë¥˜ì˜ anchor boxë¥¼ ë‚˜íƒ€ëƒ„\n",
        "    # ì´ê²ƒì„ enumerateì‹œì¼œ ì „ì²´ ì´ë¯¸ì§€ì˜ pixelì— ê°ê° 9ê°œì˜ anchor boxë¥¼ ê°€ì§€ê²Œ í•¨\n",
        "    # 32x32 feature mapì—ì„œëŠ” 32x32x9=9216ê°œì˜ anchor boxê°€ì§\n",
        "\n",
        "    shift_y = np.arange(0, height * feat_stride, feat_stride)\n",
        "    shift_x = np.arange(0, width * feat_stride, feat_stride)\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "    shift = np.stack((shift_y.ravel(), shift_x.ravel(),\n",
        "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
        "\n",
        "    A = anchor_base.shape[0]\n",
        "    K = shift.shape[0]\n",
        "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
        "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
        "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
        "    return anchor # (9216, 4)\n"
      ],
      "id": "cb749c00"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e1ff0bd"
      },
      "source": [
        "### Feature extractor(VGG) ì •ì˜\n"
      ],
      "id": "4e1ff0bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9911df3f"
      },
      "outputs": [],
      "source": [
        "def decom_vgg16():\n",
        "    # the 30th layer of features is relu of conv5_3\n",
        "    model = vgg16(pretrained=True)\n",
        "\n",
        "    features = list(model.features)[:30]\n",
        "    classifier = model.classifier\n",
        "\n",
        "    classifier = list(classifier)\n",
        "    del classifier[6]\n",
        "    if not use_drop:\n",
        "        del classifier[5]\n",
        "        del classifier[2]\n",
        "    classifier = nn.Sequential(*classifier)\n",
        "\n",
        "    # freeze top4 conv\n",
        "    for layer in features[:10]:\n",
        "        for p in layer.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    return nn.Sequential(*features), classifier"
      ],
      "id": "9911df3f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1cb8f86"
      },
      "source": [
        "### Faster R-CNN head ì •ì˜\n",
        "\n",
        "RoI pool í›„ì— classifier, regressor í†µê³¼"
      ],
      "id": "c1cb8f86"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce8ceb71"
      },
      "outputs": [],
      "source": [
        "class VGG16RoIHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Faster R-CNN head\n",
        "    RoI pool í›„ì— classifier, regressior í†µê³¼\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_class, roi_size, spatial_scale, classifier):\n",
        "        super(VGG16RoIHead, self).__init__()\n",
        "\n",
        "        self.classifier = classifier\n",
        "        self.cls_loc = nn.Linear(4096, n_class * 4) # bounding box regressor\n",
        "        self.score = nn.Linear(4096, n_class) # Classifier\n",
        "\n",
        "        normal_init(self.cls_loc, 0, 0.001)  # weight initialize\n",
        "        normal_init(self.score, 0, 0.01)     # weight initialize\n",
        "\n",
        "        self.n_class = n_class # ë°°ê²½ í¬í•¨í•œ class ìˆ˜\n",
        "        self.roi_size = roi_size # RoI-pooling í›„ feature mapì˜  ë†’ì´, ë„ˆë¹„\n",
        "        self.spatial_scale = spatial_scale # roi resize scale\n",
        "        self.roi = RoIPool( (self.roi_size, self.roi_size),self.spatial_scale)\n",
        "\n",
        "    def forward(self, x, rois, roi_indices):\n",
        "        # in case roi_indices is  ndarray\n",
        "        roi_indices = totensor(roi_indices).float()\n",
        "        rois = totensor(rois).float()\n",
        "        indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
        "        # NOTE: important: yx->xy\n",
        "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
        "        indices_and_rois =  xy_indices_and_rois.contiguous()\n",
        "\n",
        "        # ê° ì´ë¯¸ì§€ roi pooling\n",
        "        pool = self.roi(x, indices_and_rois)\n",
        "        # flatten\n",
        "        pool = pool.view(pool.size(0), -1)\n",
        "        # fully connected\n",
        "        fc7 = self.classifier(pool)\n",
        "        # regression\n",
        "        roi_cls_locs = self.cls_loc(fc7)\n",
        "        # softmax\n",
        "        roi_scores = self.score(fc7)\n",
        "\n",
        "\n",
        "        return roi_cls_locs, roi_scores"
      ],
      "id": "ce8ceb71"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85cec189"
      },
      "source": [
        "### Faster R-CNN ì •ì˜\n",
        "Feature Extraction : imageë¡œë¶€í„° feature map ìƒì„±\\\n",
        "Region Proposal Networks : Region of Interest ìƒì„±\\\n",
        "Localization and Classification Head : RoIì— í•´ë‹¹í•˜ëŠ” feature mapì„ ìµœì¢… detect"
      ],
      "id": "85cec189"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a04aba6"
      },
      "outputs": [],
      "source": [
        "def nograd(f):\n",
        "    def new_f(*args, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            return f(*args, **kwargs)\n",
        "    return new_f\n",
        "\n",
        "class FasterRCNN(nn.Module):\n",
        "    def __init__(self, extractor, rpn, head,\n",
        "                loc_normalize_mean = (0., 0., 0., 0.),\n",
        "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.extractor = extractor  # extractor : vgg\n",
        "        self.rpn = rpn              # rpn : region proposal network\n",
        "        self.head = head            # head : RoiHead\n",
        "\n",
        "        # mean and std\n",
        "        self.loc_normalize_mean = loc_normalize_mean\n",
        "        self.loc_normalize_std = loc_normalize_std\n",
        "        self.use_preset()\n",
        "\n",
        "    @property\n",
        "    def n_class(self): # ìµœì¢… class ê°œìˆ˜ (ë°°ê²½ í¬í•¨)\n",
        "        return self.head.n_class\n",
        "\n",
        "    # predict ì‹œ ì‚¬ìš©í•˜ëŠ” forward\n",
        "    # train ì‹œ FasterRCNNTrainerì„ ì‚¬ìš©í•˜ì—¬ FasterRcnnì— ìˆëŠ” extractor, rpn, headë¥¼ ëª¨ë“ˆë³„ë¡œ ë¶ˆëŸ¬ì™€ì„œ forward\n",
        "    def forward(self, x, scale=1.):\n",
        "        img_size = x.shape[2:]\n",
        "\n",
        "        h = self.extractor(x) # extractor í†µê³¼\n",
        "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.rpn(h, img_size, scale) # rpn í†µê³¼\n",
        "        roi_cls_locs, roi_scores = self.head(h, rois, roi_indices) # head í†µê³¼\n",
        "        return roi_cls_locs, roi_scores, rois, roi_indices\n",
        "\n",
        "    def use_preset(self): # prediction ê³¼ì • ì“°ì´ëŠ” threshold ì •ì˜\n",
        "        self.nms_thresh = 0.3\n",
        "        self.score_thresh = 0.05\n",
        "\n",
        "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
        "        bbox = list()\n",
        "        label = list()\n",
        "        score = list()\n",
        "\n",
        "        # skip cls_id = 0 because it is the background class\n",
        "        for l in range(1, self.n_class):\n",
        "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
        "            prob_l = raw_prob[:, l]\n",
        "            mask = prob_l > self.score_thresh\n",
        "            cls_bbox_l = cls_bbox_l[mask]\n",
        "            prob_l = prob_l[mask]\n",
        "            keep = nms(cls_bbox_l, prob_l,self.nms_thresh)\n",
        "            bbox.append(cls_bbox_l[keep].cpu().numpy())\n",
        "            # The labels are in [0, self.n_class - 2].\n",
        "            label.append((l - 1) * np.ones((len(keep),)))\n",
        "            score.append(prob_l[keep].cpu().numpy())\n",
        "\n",
        "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
        "        label = np.concatenate(label, axis=0).astype(np.int32)\n",
        "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
        "        return bbox, label, score\n",
        "\n",
        "    @nograd\n",
        "    def predict(self, imgs,sizes=None):\n",
        "        \"\"\"\n",
        "        ì´ë¯¸ì§€ì—ì„œ ê°ì²´ ê²€ì¶œ\n",
        "        Input : images\n",
        "        Output : bboxes, labels, scores\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        prepared_imgs = imgs\n",
        "\n",
        "        bboxes = list()\n",
        "        labels = list()\n",
        "        scores = list()\n",
        "        for img, size in zip(prepared_imgs, sizes):\n",
        "            img = totensor(img[None]).float()\n",
        "            scale = img.shape[3] / size[1]\n",
        "            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale) # self = FasterRCNN\n",
        "            # We are assuming that batch size is 1.\n",
        "            roi_score = roi_scores.data\n",
        "            roi_cls_loc = roi_cls_loc.data\n",
        "            roi = totensor(rois) / scale\n",
        "\n",
        "            # Convert predictions to bounding boxes in image coordinates.\n",
        "            # Bounding boxes are scaled to the scale of the input images.\n",
        "            mean = torch.Tensor(self.loc_normalize_mean).cuda(). repeat(self.n_class)[None]\n",
        "            std = torch.Tensor(self.loc_normalize_std).cuda(). repeat(self.n_class)[None]\n",
        "\n",
        "            roi_cls_loc = (roi_cls_loc * std + mean)\n",
        "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
        "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
        "            cls_bbox = loc2bbox(tonumpy(roi).reshape((-1, 4)),tonumpy(roi_cls_loc).reshape((-1, 4)))\n",
        "            cls_bbox = totensor(cls_bbox)\n",
        "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
        "            # clip bounding box\n",
        "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
        "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
        "\n",
        "            prob = (F.softmax(totensor(roi_score), dim=1))\n",
        "\n",
        "            bbox, label, score = self._suppress(cls_bbox, prob)\n",
        "            bboxes.append(bbox)\n",
        "            labels.append(label)\n",
        "            scores.append(score)\n",
        "\n",
        "        self.use_preset()\n",
        "        self.train()\n",
        "        return bboxes, labels, scores\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        '''\n",
        "        Optimizer ì„ ì–¸\n",
        "        '''\n",
        "        lr = learning_rate\n",
        "        params = []\n",
        "        for key, value in dict(self.named_parameters()).items():\n",
        "            if value.requires_grad:\n",
        "                if 'bias' in key:\n",
        "                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n",
        "                else:\n",
        "                    params += [{'params': [value], 'lr': lr, 'weight_decay': weight_decay}]\n",
        "        self.optimizer = torch.optim.SGD(params, momentum=0.9)\n",
        "        return self.optimizer\n",
        "\n",
        "    def scale_lr(self, decay=0.1):\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] *= decay\n",
        "        return self.optimizer\n"
      ],
      "id": "0a04aba6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39849b21"
      },
      "source": [
        "### Faster R-CNN ìƒì„±\n",
        "Extractor(VGG) + RPN + Head í•©ì¹˜ê¸°"
      ],
      "id": "39849b21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32726880"
      },
      "outputs": [],
      "source": [
        "class FasterRCNNVGG16(FasterRCNN):\n",
        "\n",
        "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
        "\n",
        "    def __init__(self, n_fg_class=10, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32] ): # n_fg_class : ë°°ê²½í¬í•¨ í•˜ì§€ ì•Šì€ class ê°œìˆ˜\n",
        "        extractor, classifier = decom_vgg16()\n",
        "\n",
        "        rpn = RegionProposalNetwork(\n",
        "            512, 512,\n",
        "            ratios=ratios,\n",
        "            anchor_scales=anchor_scales,\n",
        "            feat_stride=self.feat_stride,\n",
        "        )\n",
        "\n",
        "        head = VGG16RoIHead(\n",
        "            n_class=n_fg_class + 1,\n",
        "            roi_size=7,\n",
        "            spatial_scale=(1. / self.feat_stride),\n",
        "            classifier=classifier\n",
        "        )\n",
        "        super(FasterRCNNVGG16, self).__init__(\n",
        "            extractor,\n",
        "            rpn,\n",
        "            head,\n",
        "        )\n"
      ],
      "id": "32726880"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46202f13"
      },
      "source": [
        "### Trainer"
      ],
      "id": "46202f13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61acb16a"
      },
      "source": [
        "#### 0. util í•¨ìˆ˜ ì •ì˜\n",
        "bounding box IoU"
      ],
      "id": "61acb16a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9d25d7e"
      },
      "outputs": [],
      "source": [
        "def bbox_iou(bbox_a, bbox_b):\n",
        "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
        "        raise IndexError\n",
        "\n",
        "    #bbox_a 1ê°œì™€ bbox_b kê°œë¥¼ ë¹„êµí•´ì•¼í•˜ë¯€ë¡œ Noneì„ ì´ìš©í•´ì„œ ì°¨ì›ì„ ëŠ˜ë ¤ì„œ ì—°ì‚°í•œë‹¤.\n",
        "    # top left\n",
        "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
        "    # bottom right\n",
        "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
        "\n",
        "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
        "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
        "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
        "    return area_i / (area_a[:, None] + area_b - area_i)"
      ],
      "id": "c9d25d7e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac3ce234"
      },
      "source": [
        "#### 1. Anchor Target Creator\n",
        "Anchor boxì— í•´ë‹¹í•˜ëŠ” ground truth bounding box match\\\n",
        "Region Proposal Network loss êµ¬í•  ë•Œ ground truthë¡œ ì‚¬ìš©"
      ],
      "id": "ac3ce234"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69a1e360"
      },
      "outputs": [],
      "source": [
        "class AnchorTargetCreator(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_sample=256,\n",
        "                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,\n",
        "                 pos_ratio=0.5):\n",
        "        self.n_sample = n_sample\n",
        "        self.pos_iou_thresh = pos_iou_thresh\n",
        "        self.neg_iou_thresh = neg_iou_thresh\n",
        "        self.pos_ratio = pos_ratio\n",
        "\n",
        "    def __call__(self, bbox, anchor, img_size):\n",
        "\n",
        "        img_H, img_W = img_size\n",
        "\n",
        "        n_anchor = len(anchor) # 9216\n",
        "        inside_index = get_inside_index(anchor, img_H, img_W) # (2272,)\n",
        "        anchor = anchor[inside_index] # (2272, 4)\n",
        "        argmax_ious, label = self._create_label(\n",
        "            inside_index, anchor, bbox)\n",
        "\n",
        "        # compute bounding box regression targets\n",
        "        loc = bbox2loc(anchor, bbox[argmax_ious]) # (2272, 4)\n",
        "\n",
        "        # map up to original set of anchors\n",
        "        label = unmap(label, n_anchor, inside_index, fill=-1) # (9216,)\n",
        "        loc = unmap(loc, n_anchor, inside_index, fill=0) # (9216, 4)\n",
        "\n",
        "        return loc, label\n",
        "\n",
        "    def _create_label(self, inside_index, anchor, bbox):\n",
        "        # label) 1 :positive, 0 : negative, -1 : dont care\n",
        "        label = np.empty((len(inside_index),), dtype=np.int32)\n",
        "        label.fill(-1)\n",
        "\n",
        "        argmax_ious, max_ious, gt_argmax_ious = self._calc_ious(anchor, bbox, inside_index)\n",
        "\n",
        "        label[max_ious < self.neg_iou_thresh] = 0 # 0.3\n",
        "\n",
        "        # ê°€ì¥ iouê°€ í° ê²ƒì€ positive label\n",
        "        label[gt_argmax_ious] = 1\n",
        "\n",
        "        # positive label\n",
        "        label[max_ious >= self.pos_iou_thresh] = 1 # 0.7\n",
        "\n",
        "        # subsample positive labels if we have too many\n",
        "        n_pos = int(self.pos_ratio * self.n_sample)\n",
        "        pos_index = np.where(label == 1)[0]\n",
        "        if len(pos_index) > n_pos:\n",
        "            disable_index = np.random.choice(\n",
        "                pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
        "            label[disable_index] = -1\n",
        "\n",
        "        # subsample negative labels if we have too many\n",
        "        n_neg = self.n_sample - np.sum(label == 1)\n",
        "        neg_index = np.where(label == 0)[0]\n",
        "        if len(neg_index) > n_neg:\n",
        "            disable_index = np.random.choice(\n",
        "                neg_index, size=(len(neg_index) - n_neg), replace=False)\n",
        "            label[disable_index] = -1\n",
        "\n",
        "        return argmax_ious, label\n",
        "\n",
        "    def _calc_ious(self, anchor, bbox, inside_index):\n",
        "        # ious between the anchors and the gt boxes\n",
        "        ious = bbox_iou(anchor, bbox)\n",
        "        argmax_ious = ious.argmax(axis=1)\n",
        "        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n",
        "        gt_argmax_ious = ious.argmax(axis=0)\n",
        "        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
        "        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
        "\n",
        "        return argmax_ious, max_ious, gt_argmax_ious"
      ],
      "id": "69a1e360"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a11881f"
      },
      "source": [
        "#### 2. positive, negative sampling\n",
        "RPNì—ì„œ NMSë¥¼ ê±°ì¹œ roië“¤ì„ ground truthì™€ì˜ iouë¥¼ ë¹„êµ\\\n",
        "positive / negative sampling ìˆ˜í–‰ (ì´ 128ê°œ)\\\n",
        "sample roiì™€ gt_bboxë¥¼ ì´ìš©í•´ bbox regressionì—ì„œ regressioní•´ì•¼í•  ground truth locê°’(t_x, t_y, t_w, t_h)ì„ êµ¬í•¨"
      ],
      "id": "9a11881f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6928b27"
      },
      "outputs": [],
      "source": [
        "class ProposalTargetCreator:\n",
        "    def __init__(self,\n",
        "                 n_sample=128,\n",
        "                 pos_ratio=0.25, pos_iou_thresh=0.5,\n",
        "                 neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0\n",
        "                 ):\n",
        "        self.n_sample = n_sample\n",
        "        self.pos_ratio = pos_ratio\n",
        "        self.pos_iou_thresh = pos_iou_thresh # positive iou threshold\n",
        "        self.neg_iou_thresh_hi = neg_iou_thresh_hi # negitave iou threshold = (neg_iou_thresh_hi ~ neg_iou_thresh_lo)\n",
        "        self.neg_iou_thresh_lo = neg_iou_thresh_lo\n",
        "\n",
        "    def __call__(self, roi, bbox, label,\n",
        "                 loc_normalize_mean=(0., 0., 0., 0.),\n",
        "                 loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):\n",
        "        n_bbox, _ = bbox.shape\n",
        "\n",
        "        roi = np.concatenate((roi, bbox), axis=0)\n",
        "\n",
        "        pos_roi_per_image = np.round(self.n_sample * self.pos_ratio) # positive image ê°¯ìˆ˜ = 32\n",
        "        iou = bbox_iou(roi, bbox) # RoIì™€ bounding box IoU\n",
        "        gt_assignment = iou.argmax(axis=1)\n",
        "        max_iou = iou.max(axis=1)\n",
        "        gt_roi_label = label[gt_assignment] + 1 # class label [0, n_fg_class - 1] -> [1, n_fg_class].\n",
        "\n",
        "        # positive sample ì„ íƒ (>= pos_iou_thresh IoU)\n",
        "        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n",
        "        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
        "        if pos_index.size > 0:\n",
        "            pos_index = np.random.choice(\n",
        "                pos_index, size=pos_roi_per_this_image, replace=False)\n",
        "\n",
        "        # Negative sample ì„ íƒ [neg_iou_thresh_lo, neg_iou_thresh_hi)\n",
        "        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) &\n",
        "                             (max_iou >= self.neg_iou_thresh_lo))[0]\n",
        "        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n",
        "        neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n",
        "                                         neg_index.size))\n",
        "        if neg_index.size > 0:\n",
        "            neg_index = np.random.choice(\n",
        "                neg_index, size=neg_roi_per_this_image, replace=False)\n",
        "\n",
        "        # The indices that we're selecting (both positive and negative).\n",
        "        keep_index = np.append(pos_index, neg_index)\n",
        "        gt_roi_label = gt_roi_label[keep_index]\n",
        "        gt_roi_label[pos_roi_per_this_image:] = 0  # negative sampleì˜ label = 0\n",
        "        sample_roi = roi[keep_index] # (128, 4)\n",
        "\n",
        "        # sample roiì™€ gt_bboxë¥¼ ì´ìš©í•´ bbox regressionì—ì„œ regressioní•´ì•¼í•  ground truth locê°’(t_x, t_y, t_w, t_h) ê³„ì‚°\n",
        "        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]]) # (128, 4)\n",
        "        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)) / np.array(loc_normalize_std, np.float32))\n",
        "\n",
        "        return sample_roi, gt_roi_loc, gt_roi_label"
      ],
      "id": "e6928b27"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec2da581"
      },
      "source": [
        "#### 3. Trainer ì •ì˜\n",
        "training, loss ê³„ì‚°, checkpoint ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "id": "ec2da581"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d40ccd0b"
      },
      "outputs": [],
      "source": [
        "LossTuple = namedtuple('LossTuple', ['rpn_loc_loss', 'rpn_cls_loss',\n",
        "                                     'roi_loc_loss', 'roi_cls_loss',\n",
        "                                     'total_loss'])\n",
        "class FasterRCNNTrainer(nn.Module):\n",
        "\n",
        "    def __init__(self, faster_rcnn):\n",
        "        super(FasterRCNNTrainer, self).__init__()\n",
        "\n",
        "        self.faster_rcnn = faster_rcnn\n",
        "        self.rpn_sigma = rpn_sigma\n",
        "        self.roi_sigma = roi_sigma\n",
        "\n",
        "        # target creator create gt_bbox gt_label etc as training targets.\n",
        "        self.anchor_target_creator = AnchorTargetCreator()\n",
        "        self.proposal_target_creator = ProposalTargetCreator()\n",
        "\n",
        "        self.loc_normalize_mean = faster_rcnn.loc_normalize_mean\n",
        "        self.loc_normalize_std = faster_rcnn.loc_normalize_std\n",
        "\n",
        "        self.optimizer = self.faster_rcnn.get_optimizer()\n",
        "\n",
        "        # training ìƒíƒœ ë³´ì—¬ì£¼ëŠ” ì§€í‘œ\n",
        "        self.rpn_cm = ConfusionMeter(2) # confusion matrix for classification\n",
        "        self.roi_cm = ConfusionMeter(11)  # confusion matrix for classification\n",
        "        self.meters = {k: AverageValueMeter() for k in LossTuple._fields}  # average loss\n",
        "\n",
        "    def forward(self, imgs, bboxes, labels, scale):\n",
        "        n = bboxes.shape[0]\n",
        "\n",
        "        if n != 1:\n",
        "            raise ValueError('Currently only batch size 1 is supported.')\n",
        "\n",
        "        _, _, H, W = imgs.shape\n",
        "        img_size = (H, W)\n",
        "\n",
        "        # VGG (features extractor)\n",
        "        features = self.faster_rcnn.extractor(imgs)\n",
        "\n",
        "        # RPN (region proposal)\n",
        "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.faster_rcnn.rpn(features, img_size, scale)\n",
        "\n",
        "        # Since batch size is one, convert variables to singular form\n",
        "        bbox = bboxes[0]\n",
        "        label = labels[0]\n",
        "        rpn_score = rpn_scores[0]\n",
        "        rpn_loc = rpn_locs[0]\n",
        "        roi = rois\n",
        "\n",
        "        \"\"\"\n",
        "        sample roi =  rpnì—ì„œ nms ê±°ì¹œ 2000ê°œì˜ roië“¤ ì¤‘ positive/negative ë¹„ìœ¨ ê³ ë ¤í•´ ìµœì¢… samplingí•œ roi\n",
        "        \"\"\"\n",
        "        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(\n",
        "            roi,\n",
        "            tonumpy(bbox),\n",
        "            tonumpy(label),\n",
        "            self.loc_normalize_mean,\n",
        "            self.loc_normalize_std)\n",
        "\n",
        "        # NOTE it's all zero because now it only support for batch=1 now\n",
        "        # Faster R-CNN head (prediction head)\n",
        "        sample_roi_index = torch.zeros(len(sample_roi))\n",
        "        roi_cls_loc, roi_score = self.faster_rcnn.head(features,sample_roi,sample_roi_index)\n",
        "\n",
        "        # ------------------ RPN losses -------------------#\n",
        "        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(tonumpy(bbox),anchor,img_size)\n",
        "        gt_rpn_label = totensor(gt_rpn_label).long()\n",
        "        gt_rpn_loc = totensor(gt_rpn_loc)\n",
        "\n",
        "        # rpn bounding box regression loss\n",
        "        rpn_loc_loss = _fast_rcnn_loc_loss(rpn_loc,gt_rpn_loc,gt_rpn_label.data,self.rpn_sigma)\n",
        "        # rpn classification loss\n",
        "        rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-1)\n",
        "\n",
        "        _gt_rpn_label = gt_rpn_label[gt_rpn_label > -1]\n",
        "        _rpn_score = tonumpy(rpn_score)[tonumpy(gt_rpn_label) > -1]\n",
        "        self.rpn_cm.add(totensor(_rpn_score, False), _gt_rpn_label.data.long())\n",
        "\n",
        "        # ------------------ ROI losses (fast rcnn loss) -------------------#\n",
        "        n_sample = roi_cls_loc.shape[0]\n",
        "        roi_cls_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
        "        roi_loc = roi_cls_loc[torch.arange(0, n_sample).long().cuda(), \\\n",
        "                              totensor(gt_roi_label).long()]\n",
        "        gt_roi_label = totensor(gt_roi_label).long()\n",
        "        gt_roi_loc = totensor(gt_roi_loc)\n",
        "\n",
        "        # faster rcnn bounding box regression loss\n",
        "        roi_loc_loss = _fast_rcnn_loc_loss(\n",
        "            roi_loc.contiguous(),\n",
        "            gt_roi_loc,\n",
        "            gt_roi_label.data,\n",
        "            self.roi_sigma)\n",
        "\n",
        "        # faster rcnn classification loss\n",
        "        roi_cls_loss = nn.CrossEntropyLoss()(roi_score, gt_roi_label.cuda())\n",
        "\n",
        "        self.roi_cm.add(totensor(roi_score, False), gt_roi_label.data.long())\n",
        "\n",
        "        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n",
        "        losses = losses + [sum(losses)] # total_loss == sum(losses)\n",
        "\n",
        "        return LossTuple(*losses)\n",
        "\n",
        "    # training\n",
        "    def train_step(self, imgs, bboxes, labels, scale):\n",
        "        self.optimizer.zero_grad()\n",
        "        losses = self.forward(imgs, bboxes, labels, scale)\n",
        "        losses.total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.update_meters(losses)\n",
        "        return losses\n",
        "\n",
        "    # checkpoint ë§Œë“¤ê¸°\n",
        "    def save(self, save_optimizer=False, save_path=None):\n",
        "        save_dict = dict()\n",
        "\n",
        "        save_dict['model'] = self.faster_rcnn.state_dict()\n",
        "\n",
        "        if save_optimizer:\n",
        "            save_dict['optimizer'] = self.optimizer.state_dict()\n",
        "\n",
        "        if save_path is None:\n",
        "            save_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth'\n",
        "\n",
        "        save_dir = os.path.dirname(save_path)\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        torch.save(save_dict, save_path)\n",
        "        return save_path\n",
        "\n",
        "    # checkpoint load\n",
        "    def load(self, path, load_optimizer=True, parse_opt=False, ):\n",
        "        state_dict = torch.load(path)\n",
        "        if 'model' in state_dict:\n",
        "            self.faster_rcnn.load_state_dict(state_dict['model'])\n",
        "        else:  # legacy way, for backward compatibility\n",
        "            self.faster_rcnn.load_state_dict(state_dict)\n",
        "            return self\n",
        "        if 'optimizer' in state_dict and load_optimizer:\n",
        "            self.optimizer.load_state_dict(state_dict['optimizer'])\n",
        "        return self\n",
        "\n",
        "    def update_meters(self, losses):\n",
        "        loss_d = {k: scalar(v) for k, v in losses._asdict().items()}\n",
        "        for key, meter in self.meters.items():\n",
        "            meter.add(loss_d[key])\n",
        "\n",
        "    def reset_meters(self):\n",
        "        for key, meter in self.meters.items():\n",
        "            meter.reset()\n",
        "        self.roi_cm.reset()\n",
        "        self.rpn_cm.reset()\n",
        "\n",
        "    def get_meter_data(self):\n",
        "        return {k: v.value()[0] for k, v in self.meters.items()}\n",
        "\n",
        "\n",
        "def _smooth_l1_loss(x, t, in_weight, sigma):\n",
        "    sigma2 = sigma ** 2\n",
        "    diff = in_weight * (x - t)\n",
        "    abs_diff = diff.abs()\n",
        "    flag = (abs_diff.data < (1. / sigma2)).float()\n",
        "    y = (flag * (sigma2 / 2.) * (diff ** 2) +\n",
        "         (1 - flag) * (abs_diff - 0.5 / sigma2))\n",
        "    return y.sum()\n",
        "\n",
        "\n",
        "def _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n",
        "    # Localization loss êµ¬í•  ë•ŒëŠ” positive exampleì— ëŒ€í•´ì„œë§Œ ê³„ì‚°\n",
        "    in_weight = torch.zeros(gt_loc.shape).cuda()\n",
        "    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight).cuda()] = 1\n",
        "    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, in_weight.detach(), sigma)\n",
        "    loc_loss /= ((gt_label >= 0).sum().float())\n",
        "    return loc_loss"
      ],
      "id": "d40ccd0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b03eee6"
      },
      "source": [
        "### Train"
      ],
      "id": "2b03eee6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c9224c3"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    # Train dataset ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "#     dataset = TrainDataset()\n",
        "    annotation = os.path.join(data_dir,'train.json')\n",
        "    dataset = TrainCustom(annotation, data_dir, transforms=True)\n",
        "    print('load data')\n",
        "    dataloader = data_.DataLoader(dataset,\n",
        "                                  batch_size=1,     # only batch_size=1 support\n",
        "                                  shuffle=True,\n",
        "                                  pin_memory=False,\n",
        "                                  num_workers=0)\n",
        "\n",
        "    # faster rcnn ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
        "    print('model construct completed')\n",
        "\n",
        "    # faster rcnn trainer ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n",
        "\n",
        "    # checkpoint load\n",
        "    if train_load_path:\n",
        "        trainer.load(train_load_path)\n",
        "        print('load pretrained model from %s' % train_load_path)\n",
        "\n",
        "    lr_ = learning_rate\n",
        "    best_loss = 1000\n",
        "    for epoch in range(epochs):\n",
        "        trainer.reset_meters()\n",
        "        for ii, (img, bbox_, label_, scale) in enumerate(tqdm(dataloader)):\n",
        "\n",
        "            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
        "            trainer.train_step(img, bbox, label, float(scale))\n",
        "\n",
        "        losses = trainer.get_meter_data()\n",
        "        print(f\"Epoch #{epoch+1} loss: {losses}\")\n",
        "        if losses['total_loss'] < best_loss :\n",
        "            trainer.save()\n",
        "\n",
        "        if epoch == 9:\n",
        "            trainer.faster_rcnn.scale_lr(lr_decay)\n",
        "            lr_ = lr_ * lr_decay\n",
        "\n",
        "        if epoch == 13:\n",
        "            break"
      ],
      "id": "9c9224c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d704420"
      },
      "outputs": [],
      "source": [
        "train()"
      ],
      "id": "1d704420"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0771b1d"
      },
      "source": [
        "### Inference"
      ],
      "id": "b0771b1d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43a67b27"
      },
      "outputs": [],
      "source": [
        "def eval(dataloader, faster_rcnn):\n",
        "    outputs = []\n",
        "    for ii, (imgs, sizes) in enumerate(tqdm(dataloader)):\n",
        "        sizes = [sizes[0][0].item(), sizes[1][0].item()]\n",
        "        pred_bboxes_, pred_labels_, pred_scores_ = faster_rcnn.predict(imgs, [sizes])\n",
        "        for out in range(len(pred_bboxes_)):\n",
        "            outputs.append({'boxes':pred_bboxes_[out], 'scores': pred_scores_[out], 'labels': pred_labels_[out]})\n",
        "\n",
        "    return outputs"
      ],
      "id": "43a67b27"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c77514fc"
      },
      "outputs": [],
      "source": [
        "def inference():\n",
        "\n",
        "    # Test dataset ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "#     testset = TestDataset()\n",
        "    annotation = os.path.join(data_dir,'test.json')\n",
        "    testset = TestCustom(annotation, data_dir)\n",
        "    test_dataloader = data_.DataLoader(testset,\n",
        "                                       batch_size=1, # only batch_size=1 support\n",
        "                                       num_workers=0,\n",
        "                                       shuffle=False,\n",
        "                                       pin_memory=False\n",
        "                                       )\n",
        "    # faster rcnn ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
        "    state_dict = torch.load(inf_load_path)\n",
        "    if 'model' in state_dict:\n",
        "        faster_rcnn.load_state_dict(state_dict['model'])\n",
        "    print('load pretrained model from %s' % inf_load_path)\n",
        "\n",
        "    # evaluation\n",
        "    outputs = eval(test_dataloader, faster_rcnn)\n",
        "    score_threshold = 0.05\n",
        "    prediction_strings = []\n",
        "    file_names = []\n",
        "\n",
        "    # submission file ì‘ì„±\n",
        "    coco = COCO(os.path.join(data_dir, 'test.json'))\n",
        "    for i, output in enumerate(outputs):\n",
        "        prediction_string = ''\n",
        "        image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
        "        for box, score, label in zip(output['boxes'], output['scores'], output['labels']):\n",
        "            if score > score_threshold:\n",
        "                prediction_string += str(label) + ' ' + str(score) + ' ' + str(box[1]) + ' ' + str(\n",
        "                    box[0]) + ' ' + str(box[3]) + ' ' + str(box[2]) + ' '\n",
        "        prediction_strings.append(prediction_string)\n",
        "        file_names.append(image_info['file_name'])\n",
        "    submission = pd.DataFrame()\n",
        "    submission['PredictionString'] = prediction_strings\n",
        "    submission['image_id'] = file_names\n",
        "    submission.to_csv(\"./faster_rcnn_scratch_submission.csv\", index=False)\n",
        "\n",
        "    print(submission.head())"
      ],
      "id": "c77514fc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06b16e50"
      },
      "outputs": [],
      "source": [
        "inference()"
      ],
      "id": "06b16e50"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4f8b59f"
      },
      "source": [
        "# Reference\n",
        "https://github.com/chenyuntc/simple-faster-rcnn-pytorch \\\n",
        "https://github.com/shkim960520/faster-rcnn-for-studying"
      ],
      "id": "c4f8b59f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "935e4afc"
      },
      "source": [
        "###**ì½˜í…ì¸  ë¼ì´ì„ ìŠ¤**\n",
        "\n",
        "<font color='red'><b>**WARNING**</b></font> : **ë³¸ êµìœ¡ ì½˜í…ì¸ ì˜ ì§€ì‹ì¬ì‚°ê¶Œì€ ì¬ë‹¨ë²•ì¸ ë„¤ì´ë²„ì»¤ë„¥íŠ¸ì— ê·€ì†ë©ë‹ˆë‹¤. ë³¸ ì½˜í…ì¸ ë¥¼ ì–´ë– í•œ ê²½ë¡œë¡œë“  ì™¸ë¶€ë¡œ ìœ ì¶œ ë° ìˆ˜ì •í•˜ëŠ” í–‰ìœ„ë¥¼ ì—„ê²©íˆ ê¸ˆí•©ë‹ˆë‹¤.** ë‹¤ë§Œ, ë¹„ì˜ë¦¬ì  êµìœ¡ ë° ì—°êµ¬í™œë™ì— í•œì •ë˜ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜ ì¬ë‹¨ì˜ í—ˆë½ì„ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„ë°˜í•˜ëŠ” ê²½ìš°, ê´€ë ¨ ë²•ë¥ ì— ë”°ë¼ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ],
      "id": "935e4afc"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}